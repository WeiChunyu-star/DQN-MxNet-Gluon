{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import mxnet as mx\n",
    "import random\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import scipy.misc\n",
    "import gym\n",
    "import math\n",
    "from collections import namedtuple\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        #Articheture\n",
    "        self.batch_size = 32\n",
    "        self.image_size = 80\n",
    "        #Trickes\n",
    "        self.replay_buffer_size = 1000000\n",
    "        self.start_learning = 50000\n",
    "        self.learning_frequency = 4\n",
    "        self.skip_frame = 4\n",
    "        self.frame_len = 4\n",
    "        self.Target_update = 10000\n",
    "        self.epsilon_min = 0.1\n",
    "        self.annealing_end = 100000.\n",
    "        self.gamma = 0.99\n",
    "        self.replay_start_size = 500\n",
    "        self.no_op_max = 30\n",
    "        self.termination_reward = -10.\n",
    "        #otimization\n",
    "        self.num_episode = 1000\n",
    "        self.lr = 0.00025\n",
    "        self.gamma1 = 0.95\n",
    "        self.gamma2 = 0.95\n",
    "        self.rms_eps = 0.01\n",
    "        self.ctx = mx.gpu() #  enables gpu\n",
    "opt = Options()\n",
    "env_name = 'Assault-v0'\n",
    "env = gym.make(env_name)\n",
    "num_action = env.action_space.n\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "mx.random.seed(manualSeed)\n",
    "attrs = vars(opt)\n",
    "print (', '.join(\"%s: %s\" % item for item in attrs.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the DQN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DQN = gluon.nn.Sequential()\n",
    "with DQN.name_scope():\n",
    "    #first layer\n",
    "    DQN.add(gluon.nn.Conv2D(channels=32, kernel_size=8,strides = 4,padding = 0))\n",
    "    DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    DQN.add(gluon.nn.Activation('relu'))\n",
    "    #second layer\n",
    "    DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=4,strides = 2))\n",
    "    DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    DQN.add(gluon.nn.Activation('relu'))\n",
    "    #tird layer\n",
    "    DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=3,strides = 1))\n",
    "    DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    DQN.add(gluon.nn.Activation('relu'))\n",
    "    DQN.add(gluon.nn.Flatten())\n",
    "    #fourth layer\n",
    "    DQN.add(gluon.nn.Dense(512,activation ='relu'))\n",
    "    #fifth layer\n",
    "    DQN.add(gluon.nn.Dense(num_action,activation ='relu'))\n",
    "\n",
    "dqn = DQN\n",
    "dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=opt.ctx)\n",
    "DQN_trainer = gluon.Trainer(dqn.collect_params(),'RMSProp', \\\n",
    "                          {'learning_rate': opt.lr ,'gamma1':opt.gamma1,'gamma2': opt.gamma2,'epsilon': opt.rms_eps,'centered' : True})\n",
    "dqn.collect_params().zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Target_DQN = gluon.nn.Sequential()\n",
    "with Target_DQN.name_scope():\n",
    "    #first layer\n",
    "    Target_DQN.add(gluon.nn.Conv2D(channels=32, kernel_size=8,strides = 4,padding = 0))\n",
    "    Target_DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    Target_DQN.add(gluon.nn.Activation('relu'))\n",
    "    #second layer\n",
    "    Target_DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=4,strides = 2))\n",
    "    Target_DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    Target_DQN.add(gluon.nn.Activation('relu'))\n",
    "    #tird layer\n",
    "    Target_DQN.add(gluon.nn.Conv2D(channels=64, kernel_size=3,strides = 1))\n",
    "    Target_DQN.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    Target_DQN.add(gluon.nn.Activation('relu'))\n",
    "    Target_DQN.add(gluon.nn.Flatten())\n",
    "    #fourth layer\n",
    "    Target_DQN.add(gluon.nn.Dense(512,activation ='relu'))\n",
    "    #fifth layer\n",
    "    Target_DQN.add(gluon.nn.Dense(num_action,activation ='relu'))\n",
    "target_dqn = Target_DQN\n",
    "target_dqn.collect_params().initialize(mx.init.Normal(0.02), ctx=opt.ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class of replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward','done'))\n",
    "class Replay_Buffer():\n",
    "    def __init__(self, replay_buffer_size):\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.replay_buffer_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.replay_buffer_size\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(raw_frame, currentState = None, initial_state = False):\n",
    "    raw_frame = scipy.misc.imresize(raw_frame.mean(2), (opt.image_size,opt.image_size)).reshape([1,opt.image_size,opt.image_size]).astype(np.float32)/255.\n",
    "    if initial_state == True:\n",
    "        state = np.concatenate([raw_frame for _ in range(opt.frame_len)], axis =0)\n",
    "    else:\n",
    "        state = np.append(currentState[1:,:,:], raw_frame, axis = 0)\n",
    "    return state\n",
    "def rew_clipper(rew):\n",
    "    if rew>0.:\n",
    "        return 1.\n",
    "    elif rew<0.:\n",
    "        return -1.\n",
    "    else:\n",
    "        return 0\n",
    "l2loss = gluon.loss.L2Loss(batch_axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "render_image = False\n",
    "frame_counter = 0.\n",
    "annealing_count = 0.\n",
    "epis_count = 0.\n",
    "replay_memory = Replay_Buffer(opt.replay_buffer_size)\n",
    "value = nd.zeros(opt.batch_size,opt.ctx)\n",
    "tot_clipped_reward = np.zeros(opt.num_episode)\n",
    "tot_reward = np.zeros(opt.num_episode)\n",
    "moving_average_clipped = 0.\n",
    "moving_average = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(opt.num_episode):\n",
    "    cum_clipped_reward = 0\n",
    "    cum_reward = 0\n",
    "    next_frame = env.reset()\n",
    "    state = preprocess(next_frame, initial_state = True)\n",
    "    t = 0.\n",
    "    done = False\n",
    "    while not done:\n",
    "        previous_state = state\n",
    "        # show the frame\n",
    "        if render_image:\n",
    "            plt.imshow(next_frame);\n",
    "            plt.show()\n",
    "            display.clear_output(wait=True)\n",
    "            time.sleep(.01)\n",
    "        sample = random.random()\n",
    "        if frame_counter > opt.replay_start_size:\n",
    "            annealing_count += 1\n",
    "        if frame_counter == opt.replay_start_size:\n",
    "            print('annealing and laerning are started ')\n",
    "            \n",
    "            \n",
    "        \n",
    "        eps = np.maximum(1.-annealing_count/opt.annealing_end,opt.epsilon_min)\n",
    "        effective_eps = eps\n",
    "        if t < opt.no_op_max:\n",
    "            effective_eps = 1.\n",
    "        if sample < effective_eps:\n",
    "            action = random.randint(0, num_action - 1)\n",
    "        else:\n",
    "            data = nd.array(state.reshape([1,opt.frame_len,opt.image_size,opt.image_size]),opt.ctx)\n",
    "            action = int(nd.argmax(dqn(data),axis=1).as_in_context(mx.cpu()).asscalar())\n",
    "        #skip frame\n",
    "        rew = 0\n",
    "        for skip in range(opt.skip_frame-1):\n",
    "            next_frame, reward, done,_ = env.step(action)\n",
    "            cum_clipped_reward += rew_clipper(reward)\n",
    "            rew += reward\n",
    "                \n",
    "        next_frame_new, reward, done, _ = env.step(action)\n",
    "        cum_clipped_reward += rew_clipper(reward)\n",
    "        rew += reward\n",
    "        cum_reward += rew\n",
    "        #reward clipping\n",
    "        reward = rew_clipper(rew)\n",
    "        next_frame = np.maximum(next_frame_new,next_frame)\n",
    "        state = preprocess(next_frame, state)\n",
    "        replay_memory.push(previous_state,action,state,reward,done)\n",
    "        \n",
    "        if frame_counter > opt.replay_start_size:        \n",
    "            if frame_counter % opt.learning_frequency == 0:\n",
    "                transitions = replay_memory.sample(opt.batch_size)\n",
    "                batch = Transition(*zip(*transitions))\n",
    "                batch_state = nd.array(batch.state,opt.ctx)\n",
    "                batch_state_next = nd.array(batch.next_state,opt.ctx)\n",
    "                batch_reward = nd.array(batch.reward,opt.ctx)\n",
    "                batch_action = nd.array(batch.action,opt.ctx).astype('int32')\n",
    "                batch_done = nd.array(batch.done,opt.ctx)#.astype('int32')\n",
    "                with autograd.record():\n",
    "                    Q_sp = nd.max(target_dqn(batch_state_next),axis = 1)\n",
    "                    Q_sp = Q_sp*(nd.ones(opt.batch_size,ctx = opt.ctx)-batch_done)\n",
    "                    Q_s_array = dqn(batch_state)\n",
    "                    Q_s = nd.pick(Q_s_array,batch_action,1)\n",
    "                    #loss = nd.mean(nd.square(Q_s - (batch_reward +opt.gamma * Q_sp)))\n",
    "                    loss = nd.mean(l2loss(Q_s ,  (batch_reward + opt.gamma *Q_sp)))\n",
    "                loss.backward()\n",
    "                DQN_trainer.step(opt.batch_size)\n",
    "                \n",
    "        \n",
    "\n",
    "\n",
    "        t += 1\n",
    "        frame_counter += 1\n",
    "        if frame_counter > opt.replay_start_size:\n",
    "            if frame_counter % opt.Target_update == 0 :\n",
    "                check_point = frame_counter / (opt.Target_update *100)\n",
    "                fdqn = './saved_dqn/target_%s_%d' % (env_name,int(check_point))\n",
    "                dqn.save_params(fdqn)\n",
    "                target_dqn.load_params(fdqn, opt.ctx)\n",
    "        if done:\n",
    "            print('epis[%d],eps[%f],durat[%d],fnum=%d, cum_cl_rew = %d, cum_rew = %d,tot_cl = %d , tot = %d'\\\n",
    "                  %(epis_count,eps,t+1,frame_counter,cum_clipped_reward,cum_reward,moving_average_clipped,moving_average))\n",
    "    epis_count += 1\n",
    "    tot_clipped_reward[int(epis_count)] = cum_clipped_reward\n",
    "    tot_reward[int(epis_count)] = cum_reward\n",
    "    if epis_count > 50.:\n",
    "#         moving_average_clipped = (epis_count * moving_average_clipped + cum_clipped_reward)/(epis_count+1.)\n",
    "#         moving_average = (epis_count * moving_average + cum_reward)/(epis_count+1.)\n",
    "        moving_average_clipped = np.mean(tot_clipped_reward[int(epis_count)-1-50:int(epis_count)-1])\n",
    "        moving_average = np.mean(tot_reward[int(epis_count)-1-50:int(epis_count)-1])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandwidth = 100\n",
    "\n",
    "\n",
    "total_clipped = np.zeros(int(epis_count)-bandwidth)\n",
    "total_rew = np.zeros(int(epis_count)-bandwidth)\n",
    "for i in range(int(epis_count)-bandwidth):\n",
    "    total_clipped[i] = np.sum(tot_clipped_reward[i:i+bandwidth])/bandwidth\n",
    "    total_rew[i] = np.sum(tot_reward[i:i+bandwidth])/bandwidth\n",
    "t = np.arange(int(epis_count)-bandwidth)\n",
    "#likplt = plt.plot(t,total_clipped[0:opt.num_episode-bandwidth],\"b\", label = \"Clipped Return\")\n",
    "belplt = plt.plot(t,total_rew[0:int(epis_count)-bandwidth],\"r\", label = \"Return\")\n",
    "plt.legend()#handles[likplt,belplt])\n",
    "plt.xlabel(\"Number of Samples\")\n",
    "plt.ylabel(\"Likelihood Ratio and Bellman Error\")\n",
    "plt.show()\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
